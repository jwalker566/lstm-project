{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This project aims to develop a predictive model that can forecast the next day's SP500 closing price. To do this, a Long Short-Term Memory (LSTM) model has been implemented due to its suitability in capturing long term dependancies in sequential data - in this case, market time series data. As is typical with deep learning architectures, it also has the ability to learn complex patterns in the data and can automatically learn and adjust weights for the most relevant features.\n",
        "\n",
        "SP500 closing price data from 2014 to 2023 will be used to train and subsequently test the model. In total, the model uses nine features with ten time steps beginning on the previous day: SP500 closing price, volume, MACD, RSI, VIX, USDX, unemployment rate, effective federal funds rate and the consumer sentiment index. This data is then split in to a training, validation and test set and standardised using min-max scaling.\n",
        "\n",
        "A random search algorithim is utilised for hyperparameter tuning to optimise model performance on the validation set. On completion, the validation set and training set are merged to create a larger training set which the final model is then trained on.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MEpLprfBfnQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n",
        "\n",
        "Importing required libraries. Downgrade to tensorflow version 2.12 (2.14 is most up to date) is required to run random search algorithim for hyperparameter tuning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yMYvtA0DoU78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G86bXlKPnc6t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import sklearn\n",
        "import pandas_datareader as pdr\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12 #Avoids probelms with keras wrapper\n",
        "\n",
        "#Install instructions\n",
        "# conda install -c conda-forge tensorflow\n",
        "# conda install -c conda-forge pywavelets\n",
        "# pip install yfinance\n",
        "# conda install -c anaconda scikit-learn\n",
        "# conda install -c anaconda pandas_datareader"
      ],
      "metadata": {
        "id": "_VwnlC9Ord-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "MACD and RSI data is obtained from S&P Capital IQ and is loaded in from '**data.xlsx**'. The remainder of the data is sourced dynamically from the '**yfinance**' library and FRED using the '**pandas_datareader**' library.\n",
        "\n",
        "Individual data frames are then created for each each feature over the time period. To improve model performance, the SP500 closing price data was de-noised using a Haar wavelet transformation.\n",
        "\n",
        "The **add_timesteps** function is used to add ten time step columns to the individual feature data frames which include the value of that feature on the previous time step for ten steps.\n",
        "\n",
        "The individual data frames are then merged and rows shifted back one step so that we begin with values at t-1 and not t. The target values, which are the SP500 close prices , are then at t are then also added to the data frame. Finally, NaN values are removed.\n",
        "\n",
        "Ignore warning when running cell."
      ],
      "metadata": {
        "id": "4z92qCwMWbtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime.datetime(2014, 1, 1)\n",
        "end_date = datetime.datetime(2023, 10, 1)\n",
        "\n",
        "#Load in SP500 MACD and RSI data\n",
        "macd = pd.read_excel('data.xlsx')\n",
        "macd.set_index('Dates', inplace=True)\n",
        "macd.rename_axis('Date', inplace=True)\n",
        "macd.columns = ['MACD', 'RSI']\n",
        "rsi = pd.DataFrame(macd.pop('RSI'), columns=['RSI'])\n",
        "\n",
        "\n",
        "#Get Unemployment and EFFR\n",
        "unemployment = pdr.get_data_fred('UNRATE', start_date, end_date)\n",
        "effr = pdr.get_data_fred('EFFR', start_date, end_date)\n",
        "effr = effr.dropna()\n",
        "umcsent = pdr.get_data_fred('UMCSENT',start_date,end_date)\n",
        "\n",
        "\n",
        "#Convert Unemployment and Consumer Sentiment Index frequency to daily\n",
        "unemployment = unemployment.asfreq('D', method='ffill')\n",
        "umcsent = umcsent.asfreq('D', method='ffill' )\n",
        "\n",
        "\n",
        "#Creating individual SP500, USDX, VIX data frames with relevant columns\n",
        "sp500 = yf.download('^GSPC', start_date, end_date,progress = False)\n",
        "usdx = yf.download('DX-Y.NYB', start_date, end_date,progress=False)\n",
        "vix = yf.download('^VIX', start_date, end_date,progress = False)\n",
        "\n",
        "volume = sp500[['Volume']]\n",
        "sp500 = sp500[['Close']]\n",
        "usdx = usdx[['Close']]\n",
        "vix = vix[['Close']]\n",
        "target = sp500[['Close']]\n",
        "\n",
        "volume.columns=['Volume']\n",
        "sp500.columns=['SP500 Close']\n",
        "usdx.columns=['USDX']\n",
        "vix.columns=['VIX']\n",
        "target.columns=['Target']\n",
        "\n",
        "\n",
        "#Denoise close prices using Haar wavelet transformation\n",
        "close_prices = sp500['SP500 Close'].values\n",
        "level = 3\n",
        "coeffs = pywt.wavedec(close_prices, 'haar', level=level)\n",
        "threshold = 0.2\n",
        "denoised_coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]\n",
        "denoised_signal = pywt.waverec(denoised_coeffs, 'haar')[:len(close_prices)]\n",
        "sp500['SP500 Close'] = denoised_signal.copy()\n",
        "\n",
        "\n",
        "#Define function to add ten time steps to each feature data frame\n",
        "def add_timesteps(df, column_name, n_periods):\n",
        "    for i in range(1, n_periods + 1):\n",
        "      with pd.option_context('mode.chained_assignment', None):\n",
        "        col_name = f'{column_name} t-{i+1}'\n",
        "        df.loc[:, col_name] = df[column_name].shift(i)\n",
        "\n",
        "add_timesteps(sp500,'SP500 Close',10)\n",
        "add_timesteps(usdx,'USDX',10)\n",
        "add_timesteps(vix,'VIX',10)\n",
        "add_timesteps(macd,'MACD',10)\n",
        "add_timesteps(rsi,'RSI',10)\n",
        "add_timesteps(unemployment,'UNRATE',10)\n",
        "add_timesteps(effr,'EFFR',10)\n",
        "add_timesteps(umcsent,'UMCSENT',10)\n",
        "add_timesteps(volume,'Volume',10)\n",
        "\n",
        "\n",
        "#Create data frame using only rows where SP500 Close has a value\n",
        "df = sp500\n",
        "for dataframe in [volume,macd,rsi,vix,usdx,unemployment,effr,umcsent]:\n",
        "  df = pd.merge(df, dataframe, left_index=True, right_index=True, how='left')\n",
        "\n",
        "\n",
        "#Create final data frame where each feature is shifted back one time period\n",
        "df = df.shift(1)\n",
        "df = pd.merge(df,target,left_index=True,right_index=True,how='left')\n",
        "\n",
        "\n",
        "#Drop random rows containing na values\n",
        "df = df.dropna()\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "WD1pMTNwnf-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df** is then split in to a training, validation and test set usng a 80:10:10 split.\n"
      ],
      "metadata": {
        "id": "eIyII7awtnMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = df.iloc[:round(len(df)*0.80)]\n",
        "val = df.iloc[round(len(df)*0.8):round(len(df)*0.9)]\n",
        "test = df.iloc[round(len(df)*0.9):]\n",
        "\n"
      ],
      "metadata": {
        "id": "IIKJ22h0nt6z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features in the train, validation and test set are then scaled using the MinMaxScaler and the index is reset to add the date as a column."
      ],
      "metadata": {
        "id": "e7dR5ReJuIvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scale(train, *dfs):\n",
        "  scaler = MinMaxScaler()\n",
        "  train_scaled = scaler.fit_transform(train)\n",
        "  train_scaled = pd.DataFrame(train_scaled, columns=train.columns, index=train.index)\n",
        "\n",
        "  lst = []\n",
        "  for df in dfs:\n",
        "    scaled = scaler.transform(df)\n",
        "    scaled = pd.DataFrame(scaled, columns=df.columns, index=df.index)\n",
        "    lst.append(scaled)\n",
        "\n",
        "  return train_scaled, *lst\n",
        "\n",
        "\n",
        "train_scaled, val_scaled, test_scaled = scale(train,val,test)\n",
        "\n",
        "train_scaled.reset_index(inplace=True)\n",
        "val_scaled.reset_index(inplace=True)\n",
        "test_scaled.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "r-SwU0Q5nwao"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each scaled data set is then converted to three tensors and reshaped for input to the LSTM model. The features tensor must have dimesnions (number of rows, number of timesteps, number of features)"
      ],
      "metadata": {
        "id": "mhJq9JB8vABi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tensors(df):\n",
        "  dates = df.to_numpy()[:,0]\n",
        "  features = df.to_numpy()[:,1:100]\n",
        "  features = features.reshape(len(df),11,9)  #Although 10 timesteps were defined in the add_timesteps function, 11 are used here as using shift() on df gave an extra step\n",
        "  targets = df.to_numpy()[:,-1]\n",
        "\n",
        "  return dates, features.astype(np.float32), targets.astype(np.float32)  #astype(np.float32) fixes error which occurs when training model\n",
        "\n",
        "train_dates, train_features, train_targets = to_tensors(train_scaled)\n",
        "val_dates, val_features, val_targets = to_tensors(val_scaled)\n",
        "test_dates, test_features, test_targets = to_tensors(test_scaled)\n",
        "\n"
      ],
      "metadata": {
        "id": "PKMDtutJnyBC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model\n",
        "\n",
        "The LSTM model is constructed using one LSTM layer, two fully connected layers and an output layer with one neuron. The hyperparamters used in this model were initally set at different values than they are currently - the paramters currently in use were chosen based off of the results of the random search in the hyperparameter tuning section, and as expected, led to better performance.\n",
        "\n",
        "Early stopping is used as the validation loss for this model tends to converge quite quickly (usually within ~10 epochs). This prevents overfitting by halting learning if performance on the validation set begins to decrease even if performance on the training set is still increasing.\n",
        "\n",
        "When compiled, the model is fit to the data in the training set."
      ],
      "metadata": {
        "id": "8J3qRLgkxJAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape= (11,9)))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=20, batch_size=64, callbacks=[early_stopping])\n",
        "\n"
      ],
      "metadata": {
        "id": "drlIUZI3nzqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the model output predictions are scaled and in the form of a NumPy array, a function is defined to rescale the values to their original magnitude and return a list of predictions."
      ],
      "metadata": {
        "id": "8R17rnbm0B8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions_as_list(model,features_set):\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit_transform(train.iloc[:,-1].values.reshape(-1, 1))\n",
        "\n",
        "  predictions = model.predict(features_set).flatten()\n",
        "  predictions = predictions.reshape(-1, 1)\n",
        "  predictions = scaler.inverse_transform(predictions)\n",
        "  predictions = predictions.flatten().tolist()\n",
        "\n",
        "  return predictions\n",
        "\n",
        "predictions = get_predictions_as_list(model,val_features)\n"
      ],
      "metadata": {
        "id": "LR6EIi0rn1HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better idea of the performance of the model if it was to be implemented in a trading strategy, the **get_accuracy** function is created. This function returns the proportion of time the model correctly predicts the direction of the the next day's price movement."
      ],
      "metadata": {
        "id": "Y4IywPM80fd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(predictions,target_set):\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit_transform(train.iloc[:,-1].values.reshape(-1, 1))\n",
        "  targets = scaler.inverse_transform(target_set.reshape(-1,1)).flatten().tolist()\n",
        "\n",
        "  price_movement = [1 if targets[i] > targets[i-1] else 0 for i in range(1, len(targets))]\n",
        "  pred_movement = [1 if predictions[i] > predictions[i-1] else 0 for i in range(1, len(predictions))]\n",
        "  same = [1 if pm == pm_pred else 0 for pm, pm_pred in zip(price_movement, pred_movement)]\n",
        "\n",
        "  from collections import Counter\n",
        "  counts = Counter(same)\n",
        "  values = {count:value for count,value in counts.items()}\n",
        "\n",
        "  return (values[1] / sum(values.values()))\n"
      ],
      "metadata": {
        "id": "RadZR_20-Jpm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance on the validation set is plotted below. Visually, the model follows the target close prices well and has a low RMSE and MAE of 165 and 133 respectively. Looking at the accuracy, it is only slightly better than 50:50 at ~51%."
      ],
      "metadata": {
        "id": "x8YFjSHg3Cb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "plt.plot(val_dates,predictions)\n",
        "plt.plot(val_dates,val['Target'])\n",
        "plt.legend(['Validation Prediction', 'Validation Target'])\n",
        "plt.show()\n",
        "\n",
        "targets = val.iloc[:,-1]\n",
        "rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
        "mae = mean_absolute_error(targets, predictions)\n",
        "accuracy = get_accuracy(get_predictions_as_list(model,val_features),val_targets)\n",
        "\n",
        "print('RMSE:',rmse,' MAE:',mae,' Accuracy:',accuracy)\n"
      ],
      "metadata": {
        "id": "zDaP_wods-DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "*Note: I have found this code will only run with Tensorflow 2.12, pip install at top of notebook*\n",
        "\n",
        "A random search is used to iterate through multiple combinations of hyperparameters and evaluate the performance of each to get a good approximation of the hyperparamters which minimise the validation loss.\n",
        "\n",
        "A parameter space is defined with different combinations of hyperparamters for the random search to select from.\n",
        "\n",
        "A function is created to build an LSTM model with the same architecture as the model above and this is then wrapped as a Keras Regressor for input to **RandomizedSearchCV()**. The search then iterates over random combinations of the hyperparameters in the parameter space and returns the paramaters with the best performance.\n",
        "\n",
        "This code can take a while to run as it has to fit 100 models. The **n_iter** parameter can be changed from 20 to a lower number to speed the code up and get an idea of the output. I have included an image of the output with 20 iterations in the project file.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GfM5cnHu6u8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "parameter_space = {\n",
        "    'lstm_neurons': [50, 100, 150],\n",
        "    'dense1_neurons': [50, 100, 150],\n",
        "    'dense2_neurons': [32, 64, 128],\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'epochs': [20, 40, 60],\n",
        "    'batch_size': [16, 32, 64]\n",
        "}\n",
        "\n",
        "#Function to create model with same architecture as model above\n",
        "def create_lstm_model(lstm_neurons=100, dense1_neurons=100, dense2_neurons=64, learning_rate=0.001, epochs=20, batch_size=32):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(lstm_neurons, input_shape=(train_features.shape[1], train_features.shape[2])))\n",
        "    model.add(Dense(dense1_neurons, activation='relu'))\n",
        "    model.add(Dense(dense2_neurons, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=Adam(learning_rate=learning_rate),\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    return model\n",
        "\n",
        "#Create Keras Regressor\n",
        "keras_regressor = KerasRegressor(build_fn=create_lstm_model, verbose=0)\n",
        "\n",
        "#Create ranomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=keras_regressor,\n",
        "    param_distributions=parameter_space,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=2,\n",
        "    random_state=3)\n",
        "\n",
        "\n",
        "random_search.fit(train_features, train_targets)\n",
        "\n",
        "print(random_search.best_params_)"
      ],
      "metadata": {
        "id": "uxcJT15YSpPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Performance on Test Set\n",
        "\n",
        "With the model hyperparamters tuned, the validation set is then added into the training set."
      ],
      "metadata": {
        "id": "O2KnzVz-80dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating new train and test set\n",
        "train = df.iloc[:round(len(df)*0.90)]\n",
        "test = df.iloc[round(len(df)*0.9):]\n",
        "\n",
        "#Scale\n",
        "train_scaled, test_scaled = scale(train,test)\n",
        "train_scaled.reset_index(inplace=True)\n",
        "test_scaled.reset_index(inplace=True)\n",
        "\n",
        "#Split into tensors\n",
        "train_dates, train_features, train_targets = to_tensors(train_scaled)\n",
        "test_dates, test_features, test_targets = to_tensors(test_scaled)\n"
      ],
      "metadata": {
        "id": "3_PcP-DtIyYY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model retrained on new training set and accuracy is printed."
      ],
      "metadata": {
        "id": "MAV0OrrI9Gwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit model to new training set\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "model.fit(train_features, train_targets, epochs=20,batch_size=64,callbacks=[early_stopping])\n",
        "\n",
        "predictions = get_predictions_as_list(model,test_features)\n",
        "\n",
        "plt.plot(test_dates,predictions)\n",
        "plt.plot(test_dates,test['Target'])\n",
        "plt.legend(['Testing Predictions', 'Testing Observations'])\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy:',get_accuracy(predictions,test_targets))\n",
        "\n"
      ],
      "metadata": {
        "id": "Yvry2F4DP93T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the randomness inherent in training a model through random weight initialisation etc., the model will be slightly different every time it is fit. This can be seen by observing the shape of the plot and the value of metrics when repeatedly fitting the model. In the interest of reproducability, I have ran the above model multiple times and saved the best observed model configuration i.e the weights the model decided on, to **final_model.h5** in the project file.\n"
      ],
      "metadata": {
        "id": "LssE1hiP93jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "final_model = load_model('final_model.h5') #Load the final model\n",
        "\n",
        "final_model.fit(train_features,train_targets,epochs=20, batch_size=64, callbacks=[early_stopping])\n",
        "\n",
        "predictions = get_predictions_as_list(final_model,test_features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJzwVW5hfoMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the below plot, the model perfroms well on the test set and accurately captures the trends in price movement although at a slightly lower level. The RMSE and MAE are still low (compared against the magnitude of the actual closing price) at 298 and 277 respectively. The model accuracy is improved on that of the validation set at around ~53%."
      ],
      "metadata": {
        "id": "iViCzBnoAWe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate final model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "targets = test.iloc[:,-1]\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
        "mae = mean_absolute_error(targets, predictions)\n",
        "accuracy = get_accuracy(predictions, test_targets)\n",
        "\n",
        "plt.plot(test_dates,predictions)\n",
        "plt.plot(test_dates,test['Target'])\n",
        "plt.legend(['Prediction', 'Target'])\n",
        "plt.show()\n",
        "\n",
        "print('RMSE:',rmse,' MAE:',mae,' Accuracy:',accuracy)"
      ],
      "metadata": {
        "id": "t6W-nC_mncB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the model performance over the training, validation and test set is plotted."
      ],
      "metadata": {
        "id": "HRMbVBQpA5Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets = train.iloc[:,-1]\n",
        "val_targets = val.iloc[:,-1]\n",
        "test_targets = test.iloc[:,-1]\n",
        "\n",
        "train_predictions = get_predictions_as_list(final_model,train_features)\n",
        "val_predictions =  get_predictions_as_list(final_model,val_features)\n",
        "test_predictions = get_predictions_as_list(final_model,test_features)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_dates, train_targets)\n",
        "plt.plot(train_dates, train_predictions)\n",
        "plt.plot(val_dates, val_targets)\n",
        "plt.plot(val_dates, val_predictions)\n",
        "plt.plot(test_dates, test_targets)\n",
        "plt.plot(test_dates, test_predictions)\n",
        "plt.legend(['Training Target',\n",
        "            'Training Prediction',\n",
        "            'Validation Target',\n",
        "            'Validation Prediction',\n",
        "            'Testing Target',\n",
        "            'Testing Prediction'])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "S2k2uP9U4dlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}